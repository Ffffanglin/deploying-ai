{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7QNjZxviJgT6KEasX10v'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets\n",
    "\n",
    "import os\n",
    "os.getenv(\"API_GATEWAY_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8d3c8734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1', \n",
    "                api_key='any value',\n",
    "                default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')})\n",
    "\n",
    "response = client.responses.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    input = 'Hello world!'\n",
    "    \n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "256159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length: 51474\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "pdf_path = \"../02_activities/documents/managing_oneself.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "document_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        document_text += text + \"\\n\"\n",
    "\n",
    "print(\"Document length:\", len(document_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv /Users/fanglingong/Downloads/DSI/deploying-ai/05_src/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0bc830ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class SummaryOutput(BaseModel):\n",
    "    Author: str\n",
    "    Title: str\n",
    "    Relevance: str\n",
    "    Summary: str\n",
    "    Tone: str\n",
    "    InputTokens: int\n",
    "    OutputTokens: int\n",
    "    \n",
    "\n",
    "instructions = (\n",
    "    \"You are a professional summarization assistant.\\n\"\n",
    "    \"Return ONLY valid JSON.\\n\"\n",
    "    \"Do NOT include markdown formatting.\\n\"\n",
    "    \"The JSON must contain these fields:\\n\"\n",
    "    \"- Author (string)\\n\"\n",
    "    \"- Title (string)\\n\"\n",
    "    \"- Relevance (string, max one paragraph)\\n\"\n",
    "    \"- Summary (string, max 1000 tokens)\\n\"\n",
    "    \"- Tone (string)\\n\"\n",
    "    \"- InputTokens (integer, put 0 for now)\\n\"\n",
    "    \"- OutputTokens (integer, put 0 for now)\"\n",
    ")\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Document Content:\n",
    "{document_text[:5000]}\n",
    "\n",
    "- Output: structured fields Author, Title, Relevance, Summary, Tone, InputTokens, OutputTokens\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\"role\": \"developer\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    max_output_tokens=1000\n",
    ")\n",
    "\n",
    "raw_output = response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b382e76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author='Peter F. Drucker' Title='Managing Oneself' Relevance='This article emphasizes the importance of self-awareness in achieving success in the modern knowledge economy. It encourages individuals to take charge of their careers by understanding their strengths, values, and preferred working styles.' Summary=\"In 'Managing Oneself', Peter F. Drucker argues that success in today's knowledge economy requires individuals to know themselves deeply. He stresses that with the unprecedented opportunities available, individuals must take responsibility for their own career management, acting as their own chief executive officers. To thrive, one must identify personal strengths and weaknesses, understand their preferred working styles, clarify their values, and determine the best work environment for their contributions. Drucker suggests using feedback analysis to track decision outcomes and identify patterns in performance. He encourages focusing on strengths rather than trying to improve areas of weakness. Additionally, aligning personal values with organizational ethics is crucial for job satisfaction and performance. Ultimately, the article advocates for self-knowledge as the foundation for achieving lasting excellence in one's career.\" Tone='Informative and motivational' InputTokens=1278 OutputTokens=247\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "clean_output = re.sub(r\"^```json\\s*|\\s*```$\", \"\", raw_output.strip(), flags=re.MULTILINE)\n",
    "\n",
    "summary_dict = json.loads(clean_output)\n",
    "summary_struct = SummaryOutput(**summary_dict)\n",
    "\n",
    "\n",
    "summary_struct.InputTokens = response.usage.input_tokens\n",
    "summary_struct.OutputTokens = response.usage.output_tokens\n",
    "\n",
    "summary_text = summary_struct.Summary\n",
    "\n",
    "print(summary_struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f3344e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class QuestionScore(BaseModel):\n",
    "    question: str\n",
    "    score: float\n",
    "    explanation: str\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    metric_name: str\n",
    "    average_score: float\n",
    "    detailed_results: List[QuestionScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c89a38fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metric(metric_name, criteria, questions, summary_text, document_text):\n",
    "    \n",
    "    formatted_questions = \"\\n\".join(\n",
    "        [f\"{i+1}. {q}\" for i, q in enumerate(questions)]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Return ONLY valid JSON:\n",
    "\n",
    "{{\n",
    "  \"metric_name\": \"{metric_name}\",\n",
    "  \"average_score\": float,\n",
    "  \"detailed_results\": [\n",
    "    {{\n",
    "      \"question\": \"string\",\n",
    "      \"score\": float,\n",
    "      \"explanation\": \"string\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Metric: {metric_name}\n",
    "Criteria: {criteria}\n",
    "\n",
    "Original Document:\n",
    "{document_text[:6000]}\n",
    "\n",
    "Summary:\n",
    "{summary_text}\n",
    "\n",
    "Questions:\n",
    "{formatted_questions}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=[\n",
    "            {\"role\": \"developer\", \"content\": \"Return only JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    raw = response.output[0].content[0].text\n",
    "    cleaned = raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    parsed = json.loads(cleaned)\n",
    "    return EvaluationResult(**parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d80c146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_questions = [\n",
    "    \"Does the summary capture the main thesis?\",\n",
    "    \"Does it include key supporting arguments?\",\n",
    "    \"Does it avoid introducing new information?\",\n",
    "    \"Is it concise and focused?\",\n",
    "    \"Does it reflect author intent accurately?\"\n",
    "]\n",
    "\n",
    "coherence_questions = [\n",
    "    \"Is the summary logically structured?\",\n",
    "    \"Do ideas flow naturally?\",\n",
    "    \"Are transitions clear?\",\n",
    "    \"Is the argument easy to follow?\",\n",
    "    \"Is it free of contradictions?\"\n",
    "]\n",
    "\n",
    "tonality_questions = [\n",
    "    \"Is the tone formal academic writing?\",\n",
    "    \"Is vocabulary scholarly?\",\n",
    "    \"Does it avoid casual language?\",\n",
    "    \"Is it objective?\",\n",
    "    \"Is tone consistent?\"\n",
    "]\n",
    "\n",
    "safety_questions = [\n",
    "    \"Is the language non-harmful?\",\n",
    "    \"Is it unbiased?\",\n",
    "    \"Is it non-offensive?\",\n",
    "    \"Is tone respectful?\",\n",
    "    \"Is it neutral?\"\n",
    "]\n",
    "\n",
    "summarization_criteria = \"Evaluate content coverage and faithfulness.\"\n",
    "coherence_criteria = \"Evaluate clarity and logical flow.\"\n",
    "tonality_criteria = \"Evaluate formal academic tone.\"\n",
    "safety_criteria = \"Evaluate harmful or biased language.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3b8d12bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_result = evaluate_metric(\n",
    "    \"Summarization\",\n",
    "    summarization_criteria,\n",
    "    summarization_questions,\n",
    "    summary_text,\n",
    "    document_text\n",
    ")\n",
    "\n",
    "coherence_result = evaluate_metric(\n",
    "    \"Coherence\",\n",
    "    coherence_criteria,\n",
    "    coherence_questions,\n",
    "    summary_text,\n",
    "    document_text\n",
    ")\n",
    "\n",
    "tonality_result = evaluate_metric(\n",
    "    \"Tonality\",\n",
    "    tonality_criteria,\n",
    "    tonality_questions,\n",
    "    summary_text,\n",
    "    document_text\n",
    ")\n",
    "\n",
    "safety_result = evaluate_metric(\n",
    "    \"Safety\",\n",
    "    safety_criteria,\n",
    "    safety_questions,\n",
    "    summary_text,\n",
    "    document_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "23146a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SummarizationScore': 4.8, 'SummarizationReason': 'See detailed_results.', 'CoherenceScore': 4.5, 'CoherenceReason': 'See detailed_results.', 'TonalityScore': 4.6, 'TonalityReason': 'See detailed_results.', 'SafetyScore': 4.8, 'SafetyReason': 'See detailed_results.'}\n"
     ]
    }
   ],
   "source": [
    "final_evaluation = {\n",
    "    \"SummarizationScore\": summarization_result.average_score,\n",
    "    \"SummarizationReason\": \"See detailed_results.\",\n",
    "    \"CoherenceScore\": coherence_result.average_score,\n",
    "    \"CoherenceReason\": \"See detailed_results.\",\n",
    "    \"TonalityScore\": tonality_result.average_score,\n",
    "    \"TonalityReason\": \"See detailed_results.\",\n",
    "    \"SafetyScore\": safety_result.average_score,\n",
    "    \"SafetyReason\": \"See detailed_results.\"\n",
    "}\n",
    "\n",
    "print(final_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4cf01e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "improvement_prompt = f\"\"\"\n",
    "Original Summary:\n",
    "{summary_text}\n",
    "\n",
    "Evaluation Feedback:\n",
    "Summarization: {summarization_result.detailed_results}\n",
    "Coherence: {coherence_result.detailed_results}\n",
    "Tonality: {tonality_result.detailed_results}\n",
    "\n",
    "Improve the summary while keeping Formal Academic Writing.\n",
    "Return ONLY JSON with the same fields as before.\n",
    "\"\"\"\n",
    "\n",
    "improved_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\"role\": \"developer\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": improvement_prompt}\n",
    "    ],\n",
    "    temperature=0.3,\n",
    "    max_output_tokens=1000\n",
    ")\n",
    "\n",
    "clean_improved = re.sub(\n",
    "    r\"^```json\\s*|\\s*```$\", \"\",\n",
    "    improved_response.output_text.strip(),\n",
    "    flags=re.MULTILINE\n",
    ")\n",
    "\n",
    "improved_dict = json.loads(clean_improved)\n",
    "improved_summary = SummaryOutput(**improved_dict)\n",
    "\n",
    "improved_summary.InputTokens = improved_response.usage.input_tokens\n",
    "improved_summary.OutputTokens = improved_response.usage.output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c5b5f619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 4.8\n",
      "After: 0.9\n"
     ]
    }
   ],
   "source": [
    "improved_summarization = evaluate_metric(\n",
    "    \"Summarization\",\n",
    "    summarization_criteria,\n",
    "    summarization_questions,\n",
    "    improved_summary.Summary,\n",
    "    document_text\n",
    ")\n",
    "\n",
    "print(\"Before:\", summarization_result.average_score)\n",
    "print(\"After:\", improved_summarization.average_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0706c7",
   "metadata": {},
   "source": [
    "Maybe the enhancement prompt accidentally broke the structured output or confused the evaluator. My model might misinterpret detailed_results as literal text to copy to generate unparseable JSON..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
